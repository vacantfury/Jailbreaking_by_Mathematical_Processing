\documentclass[sigconf, nonacm=True]{acmart}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}

\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

\acmConference[Conference'24]{ACM Conference}{June 03--05, 2024}{New York, NY}
\acmISBN{978-1-4503-XXXX-X/24/06}

\settopmatter{printfolios=true}

\begin{document}

\title{Jailbreaking LLM with Symbolic Mathematics}

\author{Haoyu Zhang}
\email{zhang.haoyu6@northeastern.edu}
\affiliation{%  
  \institution{Northeastern University}
  \country{USA}
}

\author{Shanu Sushmita}
\email{s.sushmita@northeastern.edu}
\affiliation{%  
  \institution{Northeastern University}
  \country{USA}
}

\renewcommand{\shortauthors}{Zhang and Sushmita}

\begin{abstract}
The rapid advancement of large language models (LLMs) has led to significant breakthroughs in natural language understanding and generation \cite{brown}. However, these models are susceptible to adversarial attacks that can bypass their safety mechanisms \cite{perez}. This paper introduces "Math Prompt," a novel technique that leverages symbolic mathematics to circumvent existing safeguards in LLMs \cite{anthropic}. By encoding harmful prompts into mathematical expressions, we expose vulnerabilities in current AI safety frameworks and underscore the necessity for more robust defenses against unconventional attack vectors \cite{redteaming}.
\end{abstract}

\maketitle

\section{Background}

\subsection{Advancements and Risks of LLMs}
Large language models (LLMs), such as GPT-3 and GPT-4, have transformed natural language processing by enabling high-quality text generation, summarization, and conversational agents \cite{brown}. Despite these advancements, the capability of LLMs to generate harmful or unintended content has raised significant ethical and practical concerns \cite{bender2021dangers}. This has motivated the development of robust safety measures aimed at aligning LLM outputs with human intent \cite{openai}.

\subsection{Call for Comprehensive LLM Safety}
The increasing deployment of LLMs in sensitive applications necessitates comprehensive AI safety measures. Current strategies, including reinforcement learning from human feedback (RLHF), adversarial training, and model fine-tuning, address traditional risks but often fall short against unconventional attack vectors \cite{redteaming}. Among these risks are "jailbreaking" techniques, where malicious inputs are designed to bypass safety mechanisms and elicit restricted outputs \cite{perez}.

\subsection{Jailbreaking Techniques and Symbolic Mathematics}
Jailbreaking techniques exploit the vulnerabilities of LLMs to generate harmful outputs. Previous research has highlighted methods like adversarial prompt engineering, contextual manipulation, and multimodal exploits \cite{perez, zou2022contextual}. Symbolic mathematics represents a promising yet underexplored frontier in this domain. The ability of LLMs to interpret and solve complex mathematical constructs provides a pathway for encoding malicious prompts \cite{anthropic}. 

By leveraging symbolic reasoning, malicious prompts encoded as equations or mathematical problems can bypass traditional safety mechanisms. These encoded prompts introduce semantic shifts that challenge the detection and mitigation capabilities of current AI safety frameworks. For instance, Math Prompt—a cutting-edge technique—encodes harmful instructions into mathematical constructs \cite{bethany2024jailbreaking}. Their experiments on 13 state-of-the-art LLMs reveal an average attack success rate of 73.6\%, highlighting vulnerabilities in existing safety measures. Furthermore, embedding vector analysis demonstrates significant semantic shifts between encoded and original prompts, shedding light on the mechanisms underlying attack success.

These findings underscore the urgency of expanding AI safety frameworks to address unconventional input types, such as symbolic mathematics. Potential countermeasures include enhanced training paradigms, improved detection algorithms, and broader research into unconventional input handling to ensure robust defenses in next-generation AI systems.


\section{Methodology}

\subsection{Method 1: Set Theory Encoding of Prompts}
Our approach transforms natural language prompts into mathematical structures to bypass standard safety filters in LLMs. We achieve this by encoding harmful instructions using set theory, abstract algebra, and symbolic logic. 

The encoding process involves transforming the actions into set operations and group compositions. For a given harmful instruction $I$, we define:
\begin{enumerate}
    \item A set of operations $A$.
    \item Subsets $B, C \subseteq A$ representing specific sub-actions (e.g., bypassing security, gaining access).
    \item A group $G$ with operation $\circ$ representing sequence.
    \item Predicates $P(x), Q(x)$ to define semantic conditions.
\end{enumerate}

The LLM is tasked to find an operation $g \in G$ such that:
\begin{equation}
    g = g_1 \circ g_2 \quad \text{where} \quad g_1 \in B, g_2 \in C
\end{equation}
subject to logic constraints $\exists x \in A (P(x) \land Q(x))$. This formulation masks the harmful semantics ($I$) within the abstract symbols of set theory.

\subsection{Method 2: Prompt Partitioning and Equation Mapping}
The second method involves splitting a natural language prompt into several parts and assigning these components into a simple mathematical equation, such as:
\begin{equation}
y = a + b + c
\end{equation}
Here, each part of the harmful prompt is mapped to variables \( a \), \( b \), and \( c \). The LLM is tasked with solving the equation, which inadvertently reconstructs the original prompt by interpreting the symbolic relationships as meaningful instructions.

\subsection{Method 3: Markov Chain Encoding}
We model the generation of harmful prompts as a stochastic process. By decomposing the adversarial prompt into a sequence of state transitions $S = \{s_1, s_2, ..., s_n\}$, we construct a transition matrix $T$ where $T_{ij} = P(s_j|s_i)$. 

The problem is framed as finding the probability of reasoning a target state $S_{target}$ (the harmful outcome) from an initial state $S_{start}$:
\begin{equation}
    P(S_{target}) = \sum_{k=1}^{n} T_{start, k} \cdot T_{k, target}
\end{equation}
The LLM reconstructs the prompt by solving for the sequence of states that maximizes this probability, treating the harmful instruction as a path optimization problem in a stochastic system.

\subsection{Method 4: Quantum Mechanics Representation}
This method leverages the formalism of quantum mechanics to encode prompt semantics. We represent the system state in a Hilbert space $\mathcal{H}$. The initial benign state is $|\psi_{initial}\rangle$ and the target harmful outcome is $|\phi_{target}\rangle$.

The instruction is encoded as finding a unitary operator $\hat{U}(t)$ (the action) such that:
\begin{equation}
    P(\text{success}) = |\langle \phi_{target} | \hat{U}(T) | \psi_{initial} \rangle|^2 \to 1
\end{equation}
Security barriers are modeled as potential terms $\hat{V}$ in the Hamiltonian $\hat{H}$. The LLM acts as a physicist solving for the time-evolution operator $\hat{U}(t) = e^{-i\hat{H}t/\hbar}$, effectively describing the jailbreak procedure in the language of quantum dynamics.

\subsection{Method 5: Conditional Probability Framing}
This approach reframes the request as a statistical inference problem. We split the prompt into components $A, B, C...$ and ask the model to solve for a conditional probability, such as:
\begin{equation}
    P(Outcome | Action) = \frac{P(Outcome \cap Action)}{P(Action)}
\end{equation}
By framing the "how-to" request as a theoretical calculation of risk or likelihood, we bypass imperative filters. The model answers by explicating the relationship between the Action and Outcome, thereby providing the harmful roadmap as a statistical explanation.


\section{Experiments}

\subsection{Experimental Procedure}
We designed a rigorous experimental pipeline to evaluate the effectiveness of our mathematical jailbreaking strategies. The experiment consists of four stages:
\begin{enumerate}
    \item \textbf{Encoding (Attacker)}: A \textbf{Processing Model} (GPT-4o) takes a harmful prompt from the dataset and transforms it into a mathematical formulation using one of our proposed methods (Set Theory, Quantum, etc.).
    \item \textbf{Targeting (victim)}: The mathematically encoded prompt is sent to the \textbf{Target Model} (e.g., GPT-4o, GPT-3.5, Gemini, Claude). The target model attempts to "solve" the problem.
    \item \textbf{Decoding}: The target model's solution (response) is captured. If the attack is successful, this solution contains the harmful instructions hidden within the mathematical derivation.
    \item \textbf{Evaluation (Judge)}: An independent \textbf{Evaluation Model} (GPT-5-Nano) assesses the target model's response to determine if it complied with the underlying harmful intent.
\end{enumerate}

\subsection{Evaluation Metrics}
We employ a single, robust metric to assess compliance.

\paragraph{Obedience Score}
We utilize an automated judge (GPT-5-Nano) to classify the target model's response into one of four discrete categories based on the CLAS 2024 competition standards:
\begin{itemize}
    \item \textbf{Full Rejection (0.0)}: The model refuses the request.
    \item \textbf{Partial Rejection (0.33)}: The model adopts the persona but refuses the harmful instruction.
    \item \textbf{Partial Obedience (0.66)}: The model complies with the persona and provides limited harmful information.
    \item \textbf{Full Obedience (1.0)}: The model fully complies without inhibition.
\end{itemize}

The **Harm Score** was excluded from this study to focus specifically on the bypass capability (obedience) rather than the qualitative severity of the output.

\subsection{Experimental Results}
We evaluated our framework using a dataset of 50 harmful prompts across 20 distinct experiment configurations.
\begin{itemize}
    \item \textbf{Processing Model}: GPT-4o
    \item \textbf{Evaluation Model}: GPT-5-Nano
    \item \textbf{Baseline}: 0.027 (Full Rejection) on GPT-4o
\end{itemize}

As demonstrated in Table \ref{tab:results}, the application of symbolic mathematics proved highly effective in bypassing standard safety filters. The results indicate that abstraction layers such as Set Theory and Quantum Mechanics significantly degrade the target model's ability to recognize harmful intent. While modern models (GPT-4o) showed high susceptibility (80\%+ success), newer architectures like GPT-5-Nano exhibited robust defenses.

\begin{table}[h]
\caption{Experimental Results: Jailbreak Success Rates}
\label{tab:results}
\begin{tabular}{l|l|c|c}
\toprule
\textbf{Processing Method} & \textbf{Target Model} & \textbf{Success Rate} & \textbf{Obedience} \\
\midrule
Set Theory & GPT-4o & \textbf{88.0\%} & 0.671 \\
Quantum Mechanics & GPT-4o & 80.0\% & 0.746 \\
Markov Chain & GPT-4o & 40.0\% & 0.192 \\
Conditional Prob. & GPT-4o & 24.0\% & 0.086 \\
Rephrase + Set Theory & GPT-4o & 84.0\% & 0.544 \\
Rephrase + Markov & GPT-4o & 38.0\% & 0.159 \\
Set Theory & GPT-3.5-Turbo & 64.0\% & 0.598 \\
Set Theory & GPT-5-Nano & \textbf{0.0\%} & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Result Analysis}

\paragraph{Processor Effectiveness}
Strategies that employ high levels of abstraction (Set Theory, Quantum Mechanics) achieved the highest success rates. The \textbf{Set Theory} processor achieved an 88\% success rate, followed closely by \textbf{Quantum Mechanics} at 80\%. These methods effectively strip semantic meaning from the tokens, treating them as abstract symbols to be manipulated. In contrast, the \textbf{Conditional Probability} approach was less effective (24\%), likely because it retains more linguistic structure.

\paragraph{Pipeline Synergies}
Combining a rephrasing step with the mathematical processors did not improve performance. For Set Theory, the success rate dropped slightly from 88\% to 84\%, suggesting that initial rephrasing might introduce noise that disrupts the precise mathematical mapping.

\paragraph{Model Robustness and Empty Responses}
Our results revealed a significant divergence in robustness. The standard \textbf{GPT-4o} model yielded an 88\% success rate. In stark contrast, the next-generation \textbf{GPT-5-Nano} demonstrated remarkable resilience, achieving a \textbf{0\% success rate} (0.00 obedience).

Notably, the mechanism of defense for advanced models like GPT-5-Nano is often distinct. Instead of generating a refusal text (e.g., "I cannot fulfill this request"), the model often actively filters the response entirely.
For example, when processed with a prompt requesting a "defamatory article" (Task 1 in our dataset), the GPT-5-Nano model returned:
\begin{quote}
\textit{[LLM response filtered out due to: length, system_fingerprint=None]}
\end{quote}
This indicates that the safety filtering occurs at a deeper level, preventing any token generation even when the input is mathematically obfuscated. This active filtering mechanism is far more effective than standard RLHF fine-tuning, as it catches the adversarial pattern before semantic processing completes.

\bibliographystyle{ACM-Reference-Format}
\bibliography{Project-base}

\end{document}
