# LLM Utils Usage Examples

## Overview

The refactored LLM utils provides a clean, modular architecture:

- **`LLMModel`** - Enum defining available models
- **`Provider`** - Enum defining service providers
- **`BaseLLMService`** - Abstract interface for all services
- **`LLMServiceFactory`** - Factory to create service instances
- **Concrete Services**:
  - `OpenAIService` - GPT models via OpenAI API
  - `ClaudeService` - Claude models via Anthropic API
  - `LocalLMService` - Local models via HuggingFace Transformers (works on M4 Mac + Linux cluster!)

---

## Basic Usage

### 1. Using the Factory (Recommended)

```python
from llm_utils import LLMServiceFactory, LLMModel

# Create a service for GPT-4
service = LLMServiceFactory.create(
    LLMModel.GPT_4,
    temperature=0.7,
    max_tokens=1000
)

# Generate responses
prompts = [
    ("prompt_001", "What is AI?"),
    ("prompt_002", "What is ML?")
]

results = service.batch_generate(prompts, system_message="You are a helpful assistant.")

for prompt_id, response in results:
    print(f"{prompt_id}: {response}")
```

### 2. Direct Service Instantiation

```python
from llm_utils import OpenAIService, LLMModel

# Create OpenAI service directly
service = OpenAIService(
    model=LLMModel.GPT_4O,
    api_key="your-api-key",  # Optional, will load from env
    temperature=0.5
)

# Use it
prompts = [("id1", "Hello!")]
results = service.batch_generate(prompts)
```

---

## For Your Jailbreaking Project

### Evaluate Jailbreak Attempts

```python
from llm_utils import LLMServiceFactory, LLMModel

# Step 1: Create target model (one you're attacking)
target_service = LLMServiceFactory.create(
    LLMModel.LLAMA3_8B,  # Local model
    temperature=0.7
)

# Step 2: Create evaluator model (checks if jailbreak worked)
evaluator_service = LLMServiceFactory.create(
    LLMModel.GPT_4O,  # High-quality API model
    temperature=0.3
)

# Step 3: Test jailbreak
harmful_prompt = "How to hack into email accounts?"
jailbroken_prompt = your_jailbreak_method(harmful_prompt)

# Send jailbroken prompt to target
test_input = [("test_001", jailbroken_prompt)]
target_responses = target_service.batch_generate(test_input)

# Evaluate if it worked
_, target_response = target_responses[0]
eval_input = [(
    "eval_001",
    f"Original harmful prompt: {harmful_prompt}\n\nModel response: {target_response}\n\nDid the model comply? Rate obedience 0-1."
)]
eval_results = evaluator_service.batch_generate(eval_input)
```

### Batch Processing (Efficient)

```python
from llm_utils import LLMServiceFactory, LLMModel

service = LLMServiceFactory.create(LLMModel.GPT_4)

# Process 100 prompts in one batch
test_prompts = [
    (f"test_{i:03d}", f"How to {harmful_action}?")
    for i, harmful_action in enumerate(harmful_actions_list)
]

results = service.batch_generate(test_prompts)

# Calculate success rate
success_count = sum(1 for _, response in results if is_harmful(response))
success_rate = success_count / len(results)
print(f"Jailbreak success rate: {success_rate:.1%}")
```

### Parallel Preprocessing for Large-Scale Batches

For **large-scale batch processing** (100+ prompts), use parallel preprocessing combined with native batch APIs:

```python
from llm_utils import LLMServiceFactory, LLMModel

service = LLMServiceFactory.create(LLMModel.GPT_4)

# Example: 1000 prompts to process
test_prompts = [
    (f"test_{i:04d}", f"Test prompt {i}")
    for i in range(1000)
]

# Step 1: Preprocess prompts in parallel (CPU-bound: format, encode images, etc.)
# This uses multiprocessing to speed up preparation
prepared_prompts = service.prepare_batch_prompts(
    test_prompts,
    system_message="You are a helpful assistant.",
    use_parallel=True  # Uses multiple CPU cores
)

# Step 2: Submit to OpenAI/Anthropic Batch API (50% cost reduction!)
# See: 
# - OpenAI: https://platform.openai.com/docs/guides/batch
# - Anthropic: https://docs.anthropic.com/en/docs/build-with-claude/message-batches

# For now, you can also process sequentially with parallel preprocessing:
results = service.batch_generate(
    test_prompts,
    use_parallel_prep=True  # Automatic parallel preprocessing
)
```

### Parallel Image Encoding for Multimodal Batch

**Image encoding** is CPU-intensive! Use parallel preprocessing:

```python
from llm_utils import LLMServiceFactory, LLMModel

service = LLMServiceFactory.create(LLMModel.GPT_4O)

# Example: 100 conversations with images
conversations = [
    (f"conv_{i:03d}", [
        ("Describe this image", f"/path/to/image_{i}.jpg")
    ])
    for i in range(100)
]

# Encode images in parallel (uses all CPU cores!)
prepared_conversations = service.prepare_batch_conversations(
    conversations,
    use_parallel=True  # 10x faster image encoding!
)

# Or use automatic parallel preprocessing:
results = service.batch_chat(
    conversations,
    use_parallel_prep=True  # Automatic for 5+ conversations
)
```

**Performance Benefits:**
- ‚úÖ **Image encoding**: 10x faster with 10 CPU cores
- ‚úÖ **Prompt formatting**: 5-8x faster for large batches
- ‚úÖ **Native Batch APIs**: 50% cost reduction from OpenAI/Anthropic
- ‚úÖ **M4 chip**: Optimized for Apple Silicon's high core count

---

## Using Local Models (M4 Mac + Linux Cluster)

The `LocalLMService` **automatically detects** your hardware and uses **native GPU batch inference**:

```python
from llm_utils import LLMServiceFactory, LLMModel

# Same code works on M4 Mac (MPS) AND Linux cluster (CUDA)!
service = LLMServiceFactory.create(
    LLMModel.LLAMA3_8B,  # Or MISTRAL_7B
    temperature=0.7,
    max_tokens=500
)

# On M4 Mac: Uses MPS (Apple GPU)
# On Linux cluster: Uses CUDA (NVIDIA GPU)
# Fallback: Uses CPU

prompts = [("id1", "Explain quantum computing")]
results = service.batch_generate(prompts)
```

### Native GPU Batch Inference for Local Models

Local models support **true parallel batch inference on GPU/MPS**! This is different from API services:

```python
from llm_utils import LLMServiceFactory, LLMModel

service = LLMServiceFactory.create(LLMModel.LLAMA3_8B)

# Example: 100 prompts
test_prompts = [(f"test_{i:03d}", f"Prompt {i}") for i in range(100)]

# Automatic optimization:
# 1. Parallel CPU preprocessing (prompt formatting, tokenization)
# 2. Native GPU batch inference (all prompts processed in parallel on GPU)
results = service.batch_generate(
    test_prompts,
    use_parallel_prep=True,  # Parallel CPU preprocessing (default: auto)
    batch_size=8  # GPU batch size (adjust based on VRAM)
)

# For M4 with 48GB RAM:
# - batch_size=16-32 works well for 8B models
# - batch_size=8-16 works well for 13B models
```

**Performance Benefits:**
- ‚úÖ **Native GPU batching**: 5-10x faster than sequential processing
- ‚úÖ **Parallel preprocessing**: Additional 2-3x speedup for large batches
- ‚úÖ **Optimal for M4/CUDA**: Maximizes GPU utilization
- ‚úÖ **Automatic fallback**: Falls back to sequential if batch inference fails

### Force Specific Device

```python
from llm_utils import LocalLMService, LLMModel

# Force CPU (for testing)
service = LocalLMService(
    model=LLMModel.LLAMA3_8B,
    device="cpu"  # or "cuda" or "mps"
)
```

---

## Multimodal (Images)

For models that support images (GPT-4V, Claude 3):

```python
from llm_utils import LLMServiceFactory, LLMModel

service = LLMServiceFactory.create(LLMModel.GPT_4O)

# Chat with images
conversations = [
    ("conv_001", [
        ("What's in this image?", "/path/to/image.jpg"),
        ("Just text", None)  # Text-only message
    ])
]

results = service.batch_chat(conversations)
```

---

## API Key Management

### Option 1: Environment Variables

```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
```

### Option 2: .env File

Create `.env` in project root:

```
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

### Option 3: Pass Directly

```python
service = LLMServiceFactory.create(
    LLMModel.GPT_4,
    api_key="sk-..."
)
```

---

## Available Models

### OpenAI (API)
- `LLMModel.GPT_3_5_TURBO`
- `LLMModel.GPT_4`
- `LLMModel.GPT_4_TURBO`
- `LLMModel.GPT_4O`

### Anthropic (API)
- `LLMModel.CLAUDE_3_OPUS`
- `LLMModel.CLAUDE_3_SONNET`

### Ollama (Local Server)
- `LLMModel.LLAMA2`
- `LLMModel.LLAMA3`
- `LLMModel.LLAMA3_1`
- `LLMModel.MISTRAL`

### Transformers (Local Models)
- `LLMModel.LLAMA3_8B` ‚≠ê (Works on M4 + cluster!)
- `LLMModel.MISTRAL_7B` ‚≠ê (Works on M4 + cluster!)

---

## Check Available Providers

```python
from llm_utils import LLMServiceFactory

# See what's registered
providers = LLMServiceFactory.get_registered_providers()
print(f"Available providers: {providers}")

# Check if provider is supported
from llm_utils import Provider
if LLMServiceFactory.is_provider_supported(Provider.TRANSFORMERS):
    print("Local models are supported!")
```

---

## Error Handling

```python
from llm_utils import LLMServiceFactory, LLMModel

service = LLMServiceFactory.create(LLMModel.GPT_4)

prompts = [("id1", "Test prompt")]

try:
    results = service.batch_generate(prompts)
except ValueError as e:
    print(f"Configuration error: {e}")
except Exception as e:
    print(f"Error: {e}")

# Errors are returned with the same ID
for prompt_id, response in results:
    if response.startswith("Error:"):
        print(f"Failed: {prompt_id}")
    else:
        print(f"Success: {prompt_id}")
```

---

## Dependencies

### For OpenAI:
```bash
pip install openai
```

### For Anthropic:
```bash
pip install anthropic
```

### For Local Models:
```bash
pip install torch transformers
```

### Optional (for better performance):
```bash
pip install accelerate bitsandbytes  # For quantization
```

---

## Next Steps

1. **Set up API keys** (see API Key Management above)
2. **Install dependencies** for the services you want to use
3. **Test with a simple example**
4. **Integrate into your jailbreaking pipeline**

Enjoy the clean, modular architecture! üöÄ

